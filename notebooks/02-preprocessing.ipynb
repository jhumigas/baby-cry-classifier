{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook prepares the audio data for training.\n",
    "\n",
    "**Steps:**\n",
    "1. **Load Data**: Load the `mahmudulhasan01/baby_crying_sound` dataset.\n",
    "2. **Downsample**: Convert audio from 44.1kHz to 16kHz (common for Speech/Audio models).\n",
    "3. **Split**: Create Train (80%) and Test (20%) sets.\n",
    "4. **Save**: Persist the processed dataset to disk for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Audio\n",
    "import os\n",
    "import dotenv\n",
    "import transformers\n",
    "from huggingface_hub import login\n",
    "import functools\n",
    "\n",
    "# Ensure data directory exists\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token found: True\n"
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv(dotenv.find_dotenv(usecwd=True))\n",
    "HUGGING_FACE_PAT = os.environ.get(\"HUGGING_FACE_PAT\")\n",
    "print(f\"Token found: {bool(HUGGING_FACE_PAT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HUGGING_FACE_PAT:\n",
    "    login(token=HUGGING_FACE_PAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'label'],\n",
      "    num_rows: 1313\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"mahmudulhasan01/baby_crying_sound\"\n",
    "# Load all data initially\n",
    "raw_ds = datasets.load_dataset(DATASET_PATH, split=\"train\")\n",
    "print(raw_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 2. Downsample to 16kHz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We use the `cast_column` method (or `Audio` feature) to automatically resample the audio when accessing it.\n",
    "However, to save it processed, we might want to materialize it.\n",
    "Actually, `datasets` library handles resampling on the fly if we set the Audio feature with a different sampling rate.\n",
    "Let's explicitly map it to ensure we save the resampled arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dasha/Code/gym/zoomcamps/ml-zoomcamp-projects/baby-cry-classifier/.venv/lib/python3.13/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sampling rate: 44100 (mostly)\n",
      "New sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "# We want to downsample to 16kHz.\n",
    "# We can use datasets' feature casting to handle resampling.\n",
    "# Then we use `map` to process it in batches, ensuring it's ready for saving.\n",
    "\n",
    "# 1. Define the target audio feature\n",
    "downsampled_ds = raw_ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# 2. Define a function to process batches (if we needed more complex logic)\n",
    "# Accessing the audio column will trigger the resampling due to cast_column above.\n",
    "def preprocess_batch(batch, feature_extractor):\n",
    "    # Here we could extract other features or padding if needed.\n",
    "    # For now, just accessing ensures resampling happens.\n",
    "    audio_arrays = [x[\"array\"] for x in batch[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "      audio_arrays,\n",
    "      sampling_rate=feature_extractor.sampling_rate,\n",
    "      max_length=16000, truncation=True\n",
    "    )\n",
    "    return batch\n",
    "\n",
    "# 3. Apply the mapping\n",
    "# batched=True is faster for audio processing usually\n",
    "feature_extractor = transformers.AutoFeatureExtractor.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\")\n",
    "preprocess_batch_fn = functools.partial(preprocess_batch, feature_extractor=feature_extractor)\n",
    "ds_resampled = downsampled_ds.map(preprocess_batch_fn, batched=True, batch_size=100)\n",
    "\n",
    "# Verify one sample\n",
    "print(\"Original sampling rate: 44100 (mostly)\")\n",
    "print(f\"New sampling rate: {ds_resampled[0]['audio']['sampling_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 1050\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split 80% Train, 20% Test\n",
    "# seed for reproducibility\n",
    "split_ds = ds_resampled.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"label\")\n",
    "print(split_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 4. Save to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Saving the dataset (0/1 shards):   0%|                                                                                                           | 0/1050 [00:00<?, ? examples/s]\r",
      "Saving the dataset (0/1 shards):  95%|█████████████████████████████████████████████████████████████████████████████████████████▌    | 1000/1050 [00:00<00:00, 2322.34 examples/s]\r",
      "Saving the dataset (1/1 shards): 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1050/1050 [00:00<00:00, 2322.34 examples/s]\r",
      "Saving the dataset (1/1 shards): 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1050/1050 [00:00<00:00, 2208.24 examples/s]\n",
      "\r",
      "Saving the dataset (0/1 shards):   0%|                                                                                                            | 0/263 [00:00<?, ? examples/s]\r",
      "Saving the dataset (0/1 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 263/263 [00:00<00:00, 1224.35 examples/s]\r",
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 263/263 [00:00<00:00, 1224.35 examples/s]\r",
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 263/263 [00:00<00:00, 1222.01 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: data/baby_cry_16k\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"data/baby_cry_16k\"\n",
    "split_ds.save_to_disk(SAVE_PATH)\n",
    "print(f\"Dataset saved to: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
